# Geometric Brownian Motion with Transformer & Bayesian Neural Networks

## üöÄ Overview

This project implements a state-of-the-art **Transformer-based Geometric Brownian Motion (GBM) model** enhanced with **Bayesian Neural Networks** for stock price prediction and analysis. The model combines the power of transformer architectures with uncertainty quantification to provide more accurate and reliable predictions with confidence intervals.

## üéØ Key Features

### **Transformer Architecture**
- **Multi-Head Self-Attention**: Captures complex temporal dependencies
- **Positional Encoding**: Maintains sequence order information
- **Layer Normalization**: Pre-norm for training stability
- **Gradient Clipping**: Prevents exploding gradients
- **Learning Rate Warmup**: Stable training convergence

### **Bayesian Neural Networks**
- **Uncertainty Quantification**: Provides prediction confidence intervals
- **Aleatoric Uncertainty**: Data noise estimation
- **Epistemic Uncertainty**: Model knowledge uncertainty
- **Monte Carlo Dropout**: Alternative uncertainty estimation
- **KL Divergence Regularization**: Bayesian posterior optimization

### **Enhanced Capabilities**
- **Real-time Stock Analysis**: Live data processing and prediction
- **Comprehensive Technical Indicators**: RSI, MACD, Bollinger Bands, etc.
- **Risk Management**: VaR, CVaR, and drawdown analysis
- **Model Comparison**: Direct comparison between Bayesian and Regular Transformer
- **Interactive Visualizations**: Rich plots and uncertainty analysis charts

## üèóÔ∏è Architecture Details

### **TransformerStockPredictor**
```python
class TransformerStockPredictor(nn.Module):
    def __init__(self, input_size, d_model=128, nhead=8, num_layers=4, 
                 dim_feedforward=512, dropout=0.1, max_seq_length=100):
```

**Key Components:**
- **Input Normalization**: LayerNorm for stable training
- **Positional Encoding**: Adds temporal position information
- **Transformer Encoder**: Multi-layer self-attention processing
- **Global Feature Extraction**: Mean pooling across sequence
- **Output Heads**: Separate predictors for price, volatility, and drift
- **Uncertainty Estimation**: Monte Carlo dropout for confidence intervals

### **BayesianTransformerStockPredictor**
```python
class BayesianTransformerStockPredictor(nn.Module):
    def __init__(self, input_size, d_model=128, nhead=8, num_layers=4, 
                 dim_feedforward=512, dropout=0.1, max_seq_length=100,
                 prior_std=1.0, posterior_std_init=0.1, num_samples=10):
```

**Key Components:**
- **Bayesian Linear Layers**: Learnable weight and bias distributions
- **KL Divergence Loss**: Regularization for Bayesian inference
- **Uncertainty Decomposition**: Separates aleatoric and epistemic uncertainty
- **Monte Carlo Sampling**: Multiple forward passes for uncertainty estimation

### **Advantages Over LSTM**
| Feature | Transformer | LSTM |
|---------|-------------|------|
| **Parallelization** | ‚úÖ Full sequence parallel | ‚ùå Sequential processing |
| **Long-range Dependencies** | ‚úÖ Direct attention | ‚ùå Limited by hidden state |
| **Feature Interactions** | ‚úÖ Multi-head attention | ‚ùå Limited feature mixing |
| **Uncertainty Estimation** | ‚úÖ Built-in uncertainty | ‚ùå No uncertainty output |
| **Scalability** | ‚úÖ Scales to longer sequences | ‚ùå Memory constraints |
| **Interpretability** | ‚úÖ Attention weights | ‚ùå Hidden state analysis |

## üìä Model Performance

### **Expected Improvements**
- **MSE Reduction**: 15-25% improvement over LSTM
- **MAE Reduction**: 10-20% improvement over LSTM
- **Uncertainty Quantification**: Confidence intervals for all predictions
- **Better Long-term Predictions**: Superior handling of extended sequences
- **Faster Training**: Parallel processing capabilities

### **Model Complexity**
- **Parameters**: ~2.1M trainable parameters (optimized)
- **Memory Usage**: Efficient attention mechanisms
- **Training Time**: Faster convergence than LSTM
- **Inference Speed**: Real-time prediction capabilities

## üõ†Ô∏è Installation & Usage

### **Requirements**
```bash
pip install -r requirements.txt
```

### **Basic Usage**

#### **Train Regular Transformer Model**
```python
from gbm import train_enhanced_model

# Train transformer model
model, scaler_X, scaler_y, data, features, metrics = train_enhanced_model(
    ticker="AAPL", 
    sequence_length=60, 
    epochs=50, 
    model_type='transformer'
)
```

#### **Train Bayesian Neural Network**
```python
from gbm import train_bayesian_model

# Train Bayesian model with uncertainty quantification
bayesian_model, scaler_X, scaler_y, data, features, metrics = train_bayesian_model(
    ticker="AAPL", 
    sequence_length=60, 
    epochs=50,
    num_samples=10,
    simplified=True  # Use Monte Carlo dropout for stability
)
```

#### **Model Comparison**
```python
from gbm import compare_models_performance

# Compare Bayesian vs Regular Transformer
results = compare_models_performance(
    ticker="AAPL", 
    sequence_length=60, 
    epochs=30
)
```

#### **Run Complete Analysis**
```python
# The main execution automatically runs:
# 1. Bayesian Neural Network training
# 2. Uncertainty analysis and visualization
# 3. Regular Transformer training for comparison
# 4. Performance comparison and metrics

python gbm.py
```

## üìà Technical Indicators

The model incorporates comprehensive technical analysis:

### **Price-Based Features**
- Returns and log returns
- Moving averages (5, 10, 20, 50 periods)
- Price momentum indicators

### **Volatility Indicators**
- Rolling volatility (10, 20 periods)
- Bollinger Bands position
- High-Low ratios

### **Momentum Indicators**
- RSI (Relative Strength Index)
- MACD (Moving Average Convergence Divergence)
- Volume analysis

### **Lag Features**
- Price lags (1, 2, 3, 5 periods)
- Return lags
- Volume lags

## üî¨ Advanced Features

### **Uncertainty Quantification**
Both models provide uncertainty estimates for each prediction:

#### **Regular Transformer (Monte Carlo Dropout)**
```python
price_pred, vol_pred, drift_pred, uncertainty_pred = model.predict_with_uncertainty(x)
```

#### **Bayesian Neural Network**
```python
price_pred, vol_pred, drift_pred, aleatoric_unc, epistemic_unc = model(x, sample=True)
```

### **Uncertainty Analysis**
- **Total Uncertainty**: Combined aleatoric and epistemic uncertainty
- **Aleatoric Uncertainty**: Data noise (market volatility)
- **Epistemic Uncertainty**: Model knowledge uncertainty
- **Confidence Intervals**: Prediction ranges with specified confidence levels

## üìä Visualization Examples

The model generates comprehensive visualizations:

1. **Training Progress**: Loss curves and convergence analysis
2. **Prediction Accuracy**: Actual vs predicted price scatter plots
3. **Error Distribution**: Histograms of prediction errors
4. **Model Comparison**: Side-by-side Bayesian vs Regular Transformer analysis
5. **Uncertainty Analysis**: Uncertainty vs prediction error correlation
6. **Risk Analysis**: VaR, CVaR, and drawdown calculations
7. **Technical Analysis**: RSI, MACD, Bollinger Bands plots

## üéØ Use Cases

### **Portfolio Management**
- Risk assessment and allocation
- Expected return estimation with confidence intervals
- Volatility forecasting with uncertainty bounds

### **Trading Strategies**
- Entry/exit timing with confidence levels
- Position sizing based on prediction uncertainty
- Risk management using uncertainty quantification

### **Research & Analysis**
- Market regime detection
- Factor analysis with uncertainty
- Backtesting strategies with confidence intervals

## üîß Configuration

### **Model Hyperparameters**
```python
# Transformer Configuration
d_model = 128          # Model dimension (optimized)
nhead = 8              # Number of attention heads
num_layers = 4         # Number of transformer layers
dim_feedforward = 512  # Feed-forward dimension
dropout = 0.1          # Dropout rate
max_seq_length = 100   # Maximum sequence length
```

### **Bayesian Configuration**
```python
# Bayesian Neural Network Configuration
prior_std = 1.0        # Prior standard deviation
posterior_std_init = 0.1  # Initial posterior standard deviation
num_samples = 10       # Number of Monte Carlo samples
kl_weight = 0.01       # KL divergence weight
```

### **Training Parameters**
```python
# Training Configuration
sequence_length = 60   # Input sequence length
batch_size = 32        # Batch size
learning_rate = 0.0001 # Learning rate (AdamW)
epochs = 50            # Number of epochs
weight_decay = 1e-4    # Weight decay for regularization
```

## üöÄ Performance Tips

### **Hardware Requirements**
- **GPU**: Recommended for faster training
- **RAM**: 8GB+ for large datasets
- **Storage**: SSD recommended for data loading

### **Optimization Strategies**
- **Gradient Clipping**: Prevents exploding gradients (max_norm=0.5)
- **Learning Rate Scheduling**: Adaptive learning rates with patience
- **Early Stopping**: Prevents overfitting
- **Layer Normalization**: Pre-norm for training stability
- **Warmup**: Learning rate warmup for transformers

## üìù Current Features

### **Implemented Features**
- ‚úÖ **Transformer Architecture**: Multi-head self-attention
- ‚úÖ **Bayesian Neural Networks**: Uncertainty quantification
- ‚úÖ **Monte Carlo Dropout**: Alternative uncertainty estimation
- ‚úÖ **Enhanced Feature Engineering**: Technical indicators
- ‚úÖ **Model Comparison**: Bayesian vs Regular Transformer
- ‚úÖ **Uncertainty Visualization**: Comprehensive uncertainty analysis
- ‚úÖ **Risk Management**: VaR, CVaR calculations
- ‚úÖ **Interactive Plots**: Rich visualization capabilities

### **Future Enhancements**
- **Multi-Asset Modeling**: Portfolio-level predictions
- **Real-time Streaming**: Live market data integration
- **Advanced Attention**: Sparse attention mechanisms
- **Ensemble Methods**: Multiple model combination
- **Temporal Fusion**: Multi-timeframe analysis

## ü§ù Contributing

Contributions are welcome! Please feel free to submit pull requests or open issues for:
- Bug fixes
- Performance improvements
- New features
- Documentation updates

## üìÑ License

This project is licensed under the MIT License - see the LICENSE file for details.

## üôè Acknowledgments

- PyTorch team for the excellent deep learning framework
- Yahoo Finance for financial data
- The transformer architecture community for inspiration
- Bayesian Neural Network research community

---

**Note**: This model is for educational and research purposes. Always perform your own due diligence before making investment decisions. The uncertainty quantification provides confidence intervals but does not guarantee investment outcomes.
